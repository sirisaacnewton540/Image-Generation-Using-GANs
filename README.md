# Image-Generation-Using-GANs

This repository contains a Generative Adversarial Network (GAN) implemented in TensorFlow/Keras to generate synthetic images of handwritten digits using the MNIST dataset. The project demonstrates the application of advanced machine learning techniques, focusing on the interaction between a generator and a discriminator through adversarial training.

## Project Overview

The project explores the use of GANs to generate images that resemble the MNIST dataset's handwritten digits. The GAN consists of two main components:
- **Generator:** Learns to create images from random noise.
- **Discriminator:** Learns to distinguish between real images (from the dataset) and fake images (generated by the Generator).

### Mathematical Foundation

1. **Adversarial Loss Function:**  
   The GAN is trained using a minimax game, where the generator \(G\) tries to minimize the probability that the discriminator \(D\) correctly classifies its outputs as fake:
 
   ##               $`\min_G \max_D \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]`$
   
   The discriminator is optimized to maximize the probability of correctly identifying real versus generated images.

3. **Leaky ReLU Activation:**  
   The Leaky ReLU activation function,
   ##  $\(f(x) = \max(\alpha x, x)\)$
   where $\(\alpha\)$ is a small constant (e.g., 0.2), is used in both the generator and discriminator to allow a small gradient when the unit is not active, mitigating the issue of "dead neurons."

5. **Dropout Regularization:**  
   Dropout is applied in the discriminator to prevent overfitting by randomly setting a fraction of the input units to zero during training. This encourages the model to learn more robust features.

6. **Adam Optimizer:**  
   The Adam optimizer, which combines the advantages of Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), is used for training both the generator and the discriminator. The optimization process is governed by the learning rate, $\(\alpha = 0.0002\)$, and $\(\beta_1 = 0.5\)$, which control the exponential decay rates for the moment estimates.

## Model Architecture

### Generator
- **Input:** 100-dimensional noise vector drawn from a standard normal distribution.
- **Layers:**
  - Dense layers progressively expand the noise vector.
  - Leaky ReLU activation to maintain non-linearity.
  - Final Dense layer with a Tanh activation function reshapes the output into a 28x28 image.
  
### Discriminator
- **Input:** 28x28 grayscale image.
- **Layers:**
  - Flatten followed by Dense layers for feature extraction.
  - Leaky ReLU activation to preserve gradient flow.
  - Dropout layers to improve generalization.
  - Final Dense layer with a Sigmoid activation to output a probability score indicating real or fake.

## Training Process

1. **Adversarial Training Loop:**  
   - The discriminator is trained on batches of real images and synthetic images generated by the generator. It outputs a probability that the input is real.
   - The generator is trained to fool the discriminator by generating increasingly realistic images, effectively minimizing the adversarial loss function.

2. **Data Augmentation:**  
   - Techniques like noise injection are used to augment the training process, improving the generalization capability of the generator.

3. **Physics-Informed Constraints:**  
   - Though not included in the basic implementation, this project can be extended by incorporating constraints that enforce adherence to certain physical laws or properties within the generated images.

## Results and Applications

### After 1000 Epochs
![1000](https://github.com/user-attachments/assets/7d12dd83-0316-4a79-bef7-35d3f6433187)

### After 2000 Epochs
![2000](https://github.com/user-attachments/assets/cbaa9836-4c7e-487b-b644-ce3b4b3a9996)


### After 10000 Epochs
![10000 epochs](https://github.com/user-attachments/assets/982e55df-c394-4067-8bb1-ef92e971851f)


The GAN successfully learns to generate realistic handwritten digits after sufficient training epochs. The resulting model has potential applications in data augmentation, where synthetic data can be used to supplement real datasets, improving the performance of other machine learning models.

**Generated Images:**
    Images are saved at intervals during training and stored in the working directory.

## Future Work

- **Integration with Physics-Informed Learning:** Implement constraints based on physical principles to ensure that generated data adheres to specific rules, such as energy conservation in simulations.
- **Transfer Learning:** Explore the use of pre-trained models on similar datasets to accelerate convergence.

## License

This project is licensed under the MIT License.

---

This README provides a deeper dive into the mathematical and algorithmic principles behind your project, showcasing your expertise and the complexity of the work involved.
